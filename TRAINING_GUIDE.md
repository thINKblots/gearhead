# Complete Training Guide - CUDA & ROCm

## Quick Reference

| Task | Command | GPU Type |
|------|---------|----------|
| Train on NVIDIA GPU | `make train-small` | CUDA |
| Train on AMD GPU | `make train-small-rocm` | ROCm |
| Detect GPU | `make detect-gpu` | Any |
| Monitor GPU | `watch -n 1 rocm-smi` | ROCm |
| Monitor GPU | `watch -n 1 nvidia-smi` | CUDA |

## Installation

### For NVIDIA GPUs (CUDA)
```bash
make install
```

### For AMD GPUs (ROCm)
```bash
make install-rocm
```

## Memory Optimization

Your system: **32GB RAM**

### Problem
The original config uses ~32GB+:
- Batch size: 16
- Sequence length: 1024
- Gradient accumulation: 2

### Solution: ROCm Optimized Config

[configs/small_config_rocm.yaml](configs/small_config_rocm.yaml):
```yaml
batch_size: 4                    # 75% less memory
max_seq_length: 512              # 50% less memory
gradient_accumulation_steps: 8   # Maintains effective batch size
fp16: true                       # 50% less memory
```

**Result**: ~5-7GB usage (85% reduction!)

## Training Commands

### AMD GPU (RX 6750 XT)

```bash
# Activate your virtual environment first
source rocm/bin/activate

# Run training
make train-small-rocm
```

The script automatically:
- âœ… Detects GPU architecture (gfx1031)
- âœ… Sets HSA_OVERRIDE_GFX_VERSION
- âœ… Selects discrete GPU (not integrated)
- âœ… Configures memory allocator
- âœ… Enables ROCm optimizations

### NVIDIA GPU

```bash
# Activate your virtual environment first
source venv/bin/activate  # or wherever your venv is

# Run training
make train-small
```

## Monitoring Training

### Terminal 1: Run Training
```bash
source rocm/bin/activate
make train-small-rocm
```

### Terminal 2: Monitor GPU
```bash
# AMD GPU
watch -n 1 'rocm-smi --showmeminfo vram --showuse'

# NVIDIA GPU
watch -n 1 'nvidia-smi'
```

### Terminal 3: Monitor System
```bash
htop
```

## Expected Output

```
Detected GPU architecture: gfx1031
Environment configured for ROCm training
HSA_OVERRIDE_GFX_VERSION=gfx1031
HIP_VISIBLE_DEVICES=0

Gearhead Training
============================================================

Loading tokenizer from tokenizer/tokenizer.json...
Tokenizer loaded. Vocabulary size: 32000

Initializing model...
Model initialized with 125,789,696 parameters

Loading training data from data/processed/train.jsonl...
Training samples: 1000

Initializing trainer...
Device: cuda (ROCM: AMD Radeon RX 6750 XT)
ROCm optimizations: Enabled
Making model parameters contiguous for ROCm compatibility...

============================================================
Starting training...
============================================================

Starting training for 20 epochs
Total training steps: 62500
Device: cuda (ROCM: AMD Radeon RX 6750 XT)
ROCm optimizations: Enabled

Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:15<00:00]
```

## Troubleshooting

### Issue: "HIP error: invalid device function"

**Fixed!** The wrapper script now automatically configures your GPU.

If you still see this:
```bash
# Check GPU detection
rocm-smi --showproductname

# Manually set if needed
export HSA_OVERRIDE_GFX_VERSION=10.3.1
make train-small-rocm
```

### Issue: "No module named 'torch'"

**Cause**: Virtual environment not activated

**Solution**:
```bash
source rocm/bin/activate
make train-small-rocm
```

### Issue: Still out of memory

**Solution**: Further reduce memory usage in [configs/small_config_rocm.yaml](configs/small_config_rocm.yaml):

```yaml
batch_size: 2                    # Even smaller
gradient_accumulation_steps: 16  # Compensate
max_seq_length: 256              # Shorter sequences
```

### Issue: Slow training

**Solutions**:
1. Check GPU utilization: `rocm-smi` (should be 90%+)
2. Verify PCIe connection: `lspci | grep VGA`
3. Ensure fp16 is enabled: `fp16: true`
4. Check you're using discrete GPU (GPU 0, not GPU 1)

## GPU Selection (Your System)

You have 2 GPUs:
- **GPU 0**: AMD Radeon RX 6750 XT (gfx1031) - **Use for training** âœ…
- **GPU 1**: AMD Radeon Graphics (integrated) - Don't use âŒ

The script automatically selects GPU 0 via `HIP_VISIBLE_DEVICES=0`.

## Performance Expectations

### RX 6750 XT (12GB VRAM)

| Metric | Value |
|--------|-------|
| Memory usage | ~5-7 GB |
| Tokens/sec | ~5000-7000 |
| Time per epoch | ~2-3 minutes |
| Total training time | ~40-60 minutes (20 epochs) |

## Configuration Comparison

| Setting | Standard | ROCm Optimized | Memory Savings |
|---------|----------|----------------|----------------|
| batch_size | 16 | 4 | 75% |
| max_seq_length | 1024 | 512 | 50% |
| gradient_accumulation | 2 | 8 | - |
| fp16 | true | true | 50% |
| **Total memory** | **~32 GB** | **~5-7 GB** | **85%** |

## Advanced: Manual Training

If you prefer manual control:

```bash
# Activate venv
source rocm/bin/activate

# Set environment
export HSA_OVERRIDE_GFX_VERSION=gfx1031
export HIP_VISIBLE_DEVICES=0
export PYTORCH_HIP_ALLOC_CONF=expandable_segments:True

# Run training
python3 scripts/train.py --config configs/small_config_rocm.yaml
```

## Files Reference

- [Makefile](Makefile) - Training commands
- [scripts/train_rocm.sh](scripts/train_rocm.sh) - ROCm wrapper script
- [configs/small_config_rocm.yaml](configs/small_config_rocm.yaml) - Memory-optimized config
- [src/gearhead/training/trainer.py](src/gearhead/training/trainer.py) - Training logic
- [docs/ROCM_TRAINING.md](docs/ROCM_TRAINING.md) - Full ROCm documentation
- [ROCM_FIX.md](ROCM_FIX.md) - HIP kernel error fix details

## Next Steps

1. âœ… Activate virtual environment: `source rocm/bin/activate`
2. âœ… Run training: `make train-small-rocm`
3. âœ… Monitor GPU: `watch -n 1 rocm-smi`
4. âœ… Wait for training to complete (~40-60 minutes)
5. âœ… Find model in `outputs/small_model_rocm/`

Happy training! ðŸš€
