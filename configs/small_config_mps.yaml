# Gearhead Small Model Configuration - Apple Silicon Optimized
# ~125M parameters - optimized for M1/M2/M3 chips
#
# This config is memory-optimized for Apple Silicon with unified memory

# Model architecture
hidden_size: 384
num_layers: 6
num_heads: 6
intermediate_size: 1536
max_seq_length: 512  # Optimized for unified memory

# Data
train_data: "data/processed/train.jsonl"
val_data: "data/processed/val.jsonl"
tokenizer: "tokenizer/tokenizer.json"

# Training hyperparameters - Memory optimized for unified memory architecture
batch_size: 4  # Balanced for unified memory
learning_rate: 5e-4
num_epochs: 20
warmup_steps: 500
gradient_accumulation_steps: 8  # Maintains effective batch size of 32
weight_decay: 0.01
max_grad_norm: 1.0

# Optimization
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-8

# Checkpointing
output_dir: "./outputs/small_model_mps"
save_steps: 500
eval_steps: 250
logging_steps: 50
save_total_limit: 3

# Mixed precision (FP16 works well on Apple Silicon)
fp16: true

# Device
device: "mps"

# Weights & Biases logging (optional)
wandb: false
wandb_project: "gearhead"
wandb_run_name: "small-model-mps-v1"
