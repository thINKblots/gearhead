# Gearhead Small Model Configuration - ROCm FAST Training
# Optimized for SPEED while maintaining model quality

# Model architecture - SMALLER for faster training
hidden_size: 256        # Reduced from 384
num_layers: 4           # Reduced from 6
num_heads: 4            # Reduced from 6
intermediate_size: 1024 # Reduced from 1536
max_seq_length: 256     # Reduced from 512 (most diagnostics fit in 256 tokens)

# Data
train_data: "data/processed/train.jsonl"
val_data: "data/processed/val.jsonl"
tokenizer: "tokenizer/tokenizer.json"

# Training hyperparameters - SPEED OPTIMIZED
batch_size: 8           # INCREASED from 2 (fits in 12GB VRAM with smaller model)
learning_rate: 1e-3     # Higher LR for faster convergence
num_epochs: 3           # Reduced from 20 (3 epochs is enough)
warmup_steps: 100       # Reduced from 500
gradient_accumulation_steps: 4  # Reduced from 16 (effective batch = 32)
weight_decay: 0.01
max_grad_norm: 1.0

# Optimization
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-8

# Checkpointing - LESS FREQUENT
output_dir: "./outputs/small_model_rocm"
save_steps: 2000        # Save less often (was 500)
eval_steps: 1000        # Eval less often (was 250)
logging_steps: 100      # Log less often (was 50)
save_total_limit: 2     # Keep fewer checkpoints

# Mixed precision
fp16: true

# ROCm-specific
use_rocm: true
rocm_optimize: true

# No W&B logging
wandb: false
