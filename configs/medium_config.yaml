# Gearhead Medium Model Configuration
# ~350M parameters - balanced performance and size

# Model architecture
hidden_size: 768
num_layers: 12
num_heads: 12
intermediate_size: 3072
max_seq_length: 2048

# Data
train_data: "data/processed/train.jsonl"
val_data: "data/processed/val.jsonl"
tokenizer: "tokenizer/tokenizer.json"

# Training hyperparameters
batch_size: 8
learning_rate: 3e-4
num_epochs: 15
warmup_steps: 1000
gradient_accumulation_steps: 4
weight_decay: 0.01
max_grad_norm: 1.0

# Optimization
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-8

# Checkpointing
output_dir: "./outputs/medium_model"
save_steps: 1000
eval_steps: 500
logging_steps: 100
save_total_limit: 3

# Mixed precision (recommended for faster training)
fp16: true

# Weights & Biases logging
wandb: false
wandb_project: "gearhead"
wandb_run_name: "medium-model-v1"
