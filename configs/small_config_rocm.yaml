# Gearhead Small Model Configuration - ROCm Optimized
# ~125M parameters - optimized for AMD GPUs with ROCm
#
# This config is memory-optimized for training on 32GB AMD GPUs

# Model architecture
hidden_size: 384
num_layers: 6
num_heads: 6
intermediate_size: 1536
max_seq_length: 512  # Reduced from 1024 for better memory efficiency

# Data
train_data: "data/processed/train.jsonl"
val_data: "data/processed/val.jsonl"
tokenizer: "tokenizer/tokenizer.json"

# Training hyperparameters - Memory optimized for 12GB VRAM
batch_size: 2  # Reduced from 4 for 12GB VRAM constraint
learning_rate: 5e-4
num_epochs: 20
warmup_steps: 500
gradient_accumulation_steps: 16  # Increased from 8 to maintain effective batch size of 32
weight_decay: 0.01
max_grad_norm: 1.0

# Optimization
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-8

# Checkpointing
output_dir: "./outputs/small_model_rocm"
save_steps: 500
eval_steps: 250
logging_steps: 50
save_total_limit: 3

# Mixed precision (recommended for ROCm)
fp16: true

# ROCm-specific settings
use_rocm: true
rocm_optimize: true

# Weights & Biases logging (optional)
wandb: false
wandb_project: "gearhead"
wandb_run_name: "small-model-rocm-v1"
